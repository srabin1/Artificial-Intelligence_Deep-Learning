{"cells":[{"cell_type":"markdown","metadata":{"id":"WVa0caPZlogN"},"source":["# Fine-Tuning LLMs with Hugging Face"]},{"cell_type":"markdown","metadata":{"id":"fT5BjFcflZAh"},"source":["## Step 1: Installing and importing the libraries"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"GLXwJqbjtPho","executionInfo":{"status":"ok","timestamp":1722468336505,"user_tz":300,"elapsed":7249,"user":{"displayName":"Sanaz Rabinia","userId":"18423678739563779329"}}},"outputs":[],"source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"eRZm_OAbs3qA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722468345880,"user_tz":300,"elapsed":7000,"user":{"displayName":"Sanaz Rabinia","userId":"18423678739563779329"}},"outputId":"10b74668-695f-4606-8a2d-ff843887c561"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n"]}],"source":["!pip install huggingface_hub"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"nAMzy_0FtaUZ","executionInfo":{"status":"ok","timestamp":1722468348299,"user_tz":300,"elapsed":276,"user":{"displayName":"Sanaz Rabinia","userId":"18423678739563779329"}}},"outputs":[],"source":["import torch\n","from trl import SFTTrainer\n","from peft import LoraConfig\n","from datasets import load_dataset\n","from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline)"]},{"cell_type":"markdown","metadata":{"id":"rz3vMSzhs-P7"},"source":["## Step 2: Loading the model"]},{"cell_type":"code","source":["llama_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path = \"aboonaji/llama2finetune-v2\",\n","                                                   quantization_config = BitsAndBytesConfig(load_in_4bit = True, bnb_4bit_compute_dtype = getattr(torch, \"float16\"), bnb_4bit_quant_type = \"nf4\"))\n","llama_model.config.use_cache = False\n","llama_model.config.pretraining_tp = 1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":477},"id":"LS8FVO0oA310","executionInfo":{"status":"error","timestamp":1722468352766,"user_tz":300,"elapsed":844,"user":{"displayName":"Sanaz Rabinia","userId":"18423678739563779329"}},"outputId":"e418449b-344f-44bf-a69e-160a71d22bab"},"execution_count":19,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"No GPU found. A GPU is needed for quantization.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-e13ddc427ecc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m llama_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path = \"aboonaji/llama2finetune-v2\",\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                    quantization_config = BitsAndBytesConfig(load_in_4bit = True, bnb_4bit_compute_dtype = getattr(torch, \"float16\"), bnb_4bit_quant_type = \"nf4\"))\n\u001b[1;32m      3\u001b[0m \u001b[0mllama_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mllama_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretraining_tp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2295\u001b[0m                     \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2296\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2297\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No GPU found. A GPU is needed for quantization.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2298\u001b[0m                 logger.info(\n\u001b[1;32m   2299\u001b[0m                     \u001b[0;34m\"The device_map was not initialized.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."]}]},{"cell_type":"markdown","metadata":{"id":"g6aWb1e7tNRS"},"source":["## Step 3: Loading the tokenizer"]},{"cell_type":"code","source":["llama_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = \"aboonaji/llama2finetune-v2\", trust_remote_code = True)\n","llama_tokenizer.pad_token = llama_tokenizer.eos_token #pad_token = end of sequence token\n","llama_tokenizer.padding_side = \"right\""],"metadata":{"id":"SLT4tsM0Hrxp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"coUlIR-ytjiF"},"source":["## Step 4: Setting the training arguments"]},{"cell_type":"code","source":["training_arguments = TrainingArguments(output_dir = \"./results\", per_device_train_batch_size = 4, max_steps = 100)"],"metadata":{"id":"pYlqX1q2KFAH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0RGLZtFwHQiZ"},"source":["## Step 5: Creating the Supervised Fine-Tuning trainer"]},{"cell_type":"code","source":["llama_sft_trainer = SFTTrainer(model = llama_model,\n","                               args = training_arguments,\n","                               train_dataset = load_dataset(path = \"aboonaji/wiki_medical_terms_llam2_format\", split = \"train\"),\n","                               tokenizer = llama_tokenizer,\n","                               peft_config = LoraConfig(task_type = \"CAUSAL_LM\", r = 64, lora_alpha = 16, lora_dropout = 0.1),\n","                               dataset_text_field = \"text\")"],"metadata":{"id":"lhBPRLMOKFkT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oSF8SHFKt1xL"},"source":["## Step 6: Training the model"]},{"cell_type":"code","source":["llama_sft_trainer.train()"],"metadata":{"id":"pkEK4IxXMAcK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XMPw6WU6vbjP"},"source":["## Step 7: Chatting with the model"]},{"cell_type":"code","source":["user_prompt = \"Please tell me about Bursitis\"\n","text_generation_pipeline = pipeline(task = \"text-generation\", model = llama_model, tokenizer = llama_tokenizer, max_length = 300)\n","model_answer = text_generation_pipeline(f\"<s>[INST] {user_prompt} [/INST]\")\n","print(model_answer[0]['generated_text'])"],"metadata":{"id":"4GicRFyVMC7a"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1QdRqUg52KXlZM6H95ALtkgsLhoUJbfqY","timestamp":1700598650361},{"file_id":"1yo2NTyRaTTr0JJ4TCIqPunY2M8Dw7tOh","timestamp":1699244453424},{"file_id":"1_49KfvLBoF85Isc4Gdoyw28Oxxdpv6Eo","timestamp":1693751472053},{"file_id":"1s1ku_N8OJPKBluQBrpcZ7AkfdAcZ_Q12","timestamp":1692693138968},{"file_id":"1578XncaDK8vPWB3kLgw410DQNCqnZVec","timestamp":1692609627275},{"file_id":"1R3-kNFWLKu7tkgOM-mD99mm2nGD5-9Lh","timestamp":1692597817214},{"file_id":"1WVSL4serOfoXztXlNQLaaUQUjeufz9SF","timestamp":1692590287814},{"file_id":"1gaFqTnMarV6oTFgtPtxVudyiiXSlC845","timestamp":1692542130771},{"file_id":"1Rem8wniY65tq9Z0W5AdDfhUtm-yFpVuX","timestamp":1692463431916},{"file_id":"1SZSeXt3LCYrzjCEfWLuUd1vZadLm4v01","timestamp":1692341216495},{"file_id":"1hbd_kGydA8pKAYuqlKUXMOw6YJEhv4Tz","timestamp":1691472982802},{"file_id":"1YoUr7SPQrbBW3ubPpofTIb-99EdQv5S0","timestamp":1691423558809},{"file_id":"1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd","timestamp":1691305989771}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}